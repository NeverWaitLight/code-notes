# Ragas 0.4.2 使用已有数据直接评估教程

## 概述

在 ragas 中，除了使用 `@experiment` 装饰器一边提问一边获取回答一边评测的方式外，还可以直接使用已经收集好的数据（包括 RAG 系统的回答和选择的引用上下文）进行评估。

## 数据准备

### 数据格式要求

评估数据需要包含以下字段：

- `user_input`: 用户的问题或查询
- `retrieved_contexts`: RAG 系统检索到的上下文列表（字符串列表）
- `response`: RAG 系统生成的回答
- `reference`: 参考答案（可选，用于有参考的评估指标）

### 示例数据

```python
dataset = [
    {
        "user_input": "什么是 Ragas?",
        "retrieved_contexts": [
            "Ragas 是一个用于评估 RAG 系统的开源框架。",
            "它提供了一系列基于 LLM 的指标。"
        ],
        "response": "Ragas 是评估 RAG 管道的工具。",
        "reference": "Ragas 是一个帮助评估集成检索增强生成 (RAG) 管道的框架。",
    },
    {
        "user_input": "Ragas 的最新版本是多少？",
        "retrieved_contexts": [
            "Ragas 正在快速迭代。",
            "当前最新版本在 2025 年末发布。"
        ],
        "response": "目前的版本是 0.4.2。",
        "reference": "Ragas 的最新版本是 0.4.2。",
    },
]
```

## 创建评估数据集

使用 `EvaluationDataset.from_list()` 方法将数据列表转换为评估数据集：

```python
from ragas import EvaluationDataset

evaluation_dataset = EvaluationDataset.from_list(dataset)
```

## 选择评估指标

ragas 提供了多种评估指标，常用的包括：

- `faithfulness`: 评估回答是否忠实于检索到的上下文
- `answer_correctness`: 评估回答的正确性（需要 reference）
- `context_precision`: 评估检索到的上下文的精确度
- `context_recall`: 评估检索到的上下文的召回率
- `answer_relevancy`: 评估回答与问题的相关性

```python
from ragas.metrics import (
    faithfulness,
    answer_correctness,
    context_precision,
    context_recall,
    answer_relevancy,
)

metrics = [
    faithfulness,
    answer_correctness,
    context_precision,
    context_recall,
    answer_relevancy,
]
```

## 执行评估

使用 `evaluate()` 函数执行评估：

```python
from ragas import evaluate

result = evaluate(
    dataset=evaluation_dataset,
    metrics=metrics,
)

print(result)
```

如果需要指定 LLM 用于评估（某些指标需要 LLM）：

```python
from ragas import evaluate
from ragas.llms import llm_factory
from openai import OpenAI

openai_client = OpenAI(api_key="your-api-key")
evaluator_llm = llm_factory("gpt-4o", client=openai_client)

result = evaluate(
    dataset=evaluation_dataset,
    metrics=metrics,
    llm=evaluator_llm,
)
```

## 查看评估结果

评估结果可以通过多种方式查看：

```python
# 转换为 pandas DataFrame
df = result.to_pandas()
print(df)

# 查看每个指标的分数
print(result.scores)

# 查看总体统计
print(result)
```

## 完整示例代码

```python
from ragas import EvaluationDataset, evaluate
from ragas.metrics import (
    faithfulness,
    answer_correctness,
    context_precision,
    context_recall,
    answer_relevancy,
)
from ragas.llms import llm_factory
from openai import OpenAI
import os

# 准备数据
dataset = [
    {
        "user_input": "什么是 Ragas?",
        "retrieved_contexts": [
            "Ragas 是一个用于评估 RAG 系统的开源框架。",
            "它提供了一系列基于 LLM 的指标。"
        ],
        "response": "Ragas 是评估 RAG 管道的工具。",
        "reference": "Ragas 是一个帮助评估集成检索增强生成 (RAG) 管道的框架。",
    },
    {
        "user_input": "Ragas 的最新版本是多少？",
        "retrieved_contexts": [
            "Ragas 正在快速迭代。",
            "当前最新版本在 2025 年末发布。"
        ],
        "response": "目前的版本是 0.4.2。",
        "reference": "Ragas 的最新版本是 0.4.2。",
    },
]

# 创建评估数据集
evaluation_dataset = EvaluationDataset.from_list(dataset)

# 配置评估指标
metrics = [
    faithfulness,
    answer_correctness,
    context_precision,
    context_recall,
    answer_relevancy,
]

# 配置 LLM（如果需要）
openai_client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
evaluator_llm = llm_factory("gpt-4o", client=openai_client)

# 执行评估
result = evaluate(
    dataset=evaluation_dataset,
    metrics=metrics,
    llm=evaluator_llm,
)

# 查看结果
print(result)
df = result.to_pandas()
print(df)
```

## 从 CSV 文件加载数据

如果数据已经存储在 CSV 文件中，可以这样加载：

```python
import pandas as pd
from ragas import EvaluationDataset

# 读取 CSV 文件
df = pd.read_csv("your_data.csv")

# 转换为字典列表格式
dataset = []
for _, row in df.iterrows():
    # 如果 retrieved_contexts 是字符串格式的列表，需要解析
    import ast
    contexts = ast.literal_eval(row["retrieved_contexts"]) if isinstance(row["retrieved_contexts"], str) else row["retrieved_contexts"]
    
    dataset.append({
        "user_input": row["user_input"],
        "retrieved_contexts": contexts,
        "response": row["response"],
        "reference": row.get("reference", None),  # 可选字段
    })

# 创建评估数据集
evaluation_dataset = EvaluationDataset.from_list(dataset)
```

## 注意事项

1. `retrieved_contexts` 必须是字符串列表，不能是单个字符串
2. `reference` 字段是可选的，但某些指标（如 `answer_correctness`）需要它
3. 某些指标需要 LLM 进行评估，需要配置 `llm` 参数
4. 评估过程可能需要一些时间，特别是使用 LLM 进行评估时

## 与 @experiment 方式的区别

- `@experiment` 方式：一边向 RAG 系统提问，一边获取回答，一边进行评测
- 直接评估方式：使用已经收集好的数据（包括回答和引用上下文）直接进行评估

直接评估方式适合以下场景：
- 已经运行过 RAG 系统并保存了结果
- 需要对比不同版本的 RAG 系统
- 需要批量评估大量数据
- 需要离线评估，不依赖实时 RAG 系统

